\begin{abstract}
%\addchaptertocentry{\abstractname} % Add the abstract to the table of contents

\noindent Generalizing to unseen environments during testing remains an open challenge for neural network training known as domain generalization. Traditionally, for most machine learning settings, gaining some degree of explainability that tries to give users more insights into \emph{how} and \emph{why} the network arrives at its predictions, restricts the underlying model and hinders performance to a certain degree. For example, decision trees are thought of as being more explainable than deep neural networks but they lack performance on visual tasks. In this work, we empirically demonstrate that applying methods and architectures from the explainability literature can, in fact, achieve state-of-the-art performance on the domain generalization task while offering a framework for more insights into the prediction process. For that, we develop a set of novel algorithms including \divcam, an approach where the network receives guidance during training via gradient based class activation maps to focus on a diverse set of discriminative features, as well as \prodrop and \dtransformers which apply prototypical networks to the domain generalization task, either with self-challenging or attention alignment. Since these methods offer competitive performance on top of explainability, we argue that they are the preferred choice for achieving domain generalization, especially in safety-critical scenarios. 

\end{abstract}