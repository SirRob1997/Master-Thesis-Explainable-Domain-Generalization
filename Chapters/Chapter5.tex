\chapter{Experiments}
In an effort to improve comparability and reproducability, we use \domainbed \citep{gulrajani2020search} for all ablation studies and experimental results. We compare to all methods currently in \domainbed which includes our provided implementation of \rsc. Further, all experiments show results for both \emph{training-domain validation} which assumes that training and testing domains have similar distributions  and \emph{oracle validation} which has limited access to the testing domain. We omit \emph{leave-one-domain-out cross-validation} as it requires the most computational resources and performs the worst out of the three validation techniques currently available in \domainbed \citep{gulrajani2020search}.

\section{Datasets and splits}
Since the size of the validation dataset can have a heavy impact on performance, we follow the design choices of \domainbed and choose $20\%$ of each domain as the validation size for all experiments and ablation studies. Here, we present results for VLCS \citep{FangXR13}, PACS \citep{LiYSH17}, Office-Home \citep{VenkateswaraECP17}, Terra Incognita \citep{BeeryHP18}, and DomainNet \citep{PengBXHSW19}. Although sometimes disregarded in the body of literature for domain generalization, we provide results for three different dataset splits to assess the stability regarding the model selection and to avoid overfitting on one split. 

\section{Hyperparameter Distributions \& Schedules}
For the main results, we use the official \domainbed hyperparameter distributions as well as the \adam optimizer with no learning rate schedule. This corresponds to the setup in which all baselines from \Cref{tab:perfom} have been evaluated in by \citet{gulrajani2020search} to provide a fair comparison. For the hyperparameters that get introduced in our methods, we choose similar distributions as used in the ablation studies from \Cref{sec:ablation}. Further, \Cref{sec:ablation} also shows the official distributions of \domainbed for all other shared hyperparameters such as learning rate $\alpha$ or batch size $\mathcal{B}$.

\section{Results}

\begin{table*}[t]
\small
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Algorithm} & \textbf{P} & \textbf{A} & \textbf{C} & \textbf{S} &  \textbf{Avg.} \\
\midrule
\divcams & 94.4 $\pm$ 0.7 & 80.5 $\pm$ 0.4 & 74.6 $\pm$ 2.2 & 79.0 $\pm$ 0.9 & 82.1 $\pm$ 0.3  \\
\prodrop & 93.6 $\pm$ 0.6 & 82.1 $\pm$ 0.9 & 76.4 $\pm$ 0.9 & 76.3 $\pm$ 0.6 & 82.1 $\pm$ 0.6 \\
\dtransformers & 93.7 $\pm$ 0.6 & 80.1 $\pm$ 1.2 & 73.3 $\pm$ 1.3 & 72.6 $\pm$ 1.5  & 79.9 $\pm$ 0.1 \\
\bottomrule
\end{tabular}
\caption[Performance comparison of the proposed methods on the PACS dataset]{Performance comparison of the proposed methods for the PACS dataset with a ResNet-18 backbone.}
\label{tab:pacs_ours}
\end{table*}



The high-level results for \divcams inside the \domainbed framework and across datasets are shown in \Cref{tab:perfom}. For completeness, we also show results outside of \domainbed on the official PACS split in \Cref{tab:official} using a ResNet-18 backbone. The full results, including the performance for choosing any domain inside each dataset as a testing domain, are shown in \Cref{sec:DomainResults}. While we are able to achieve state-of-the-art performance outside of the \domainbed framework, utilizing learning rate schedules and hyperparameter fine-tuning, we are also able to achieve Top-$4$ performance across five datasets within \domainbed. Notably, we outperform \rsc in almost every scenario, while exhibiting less standard deviation. This leads to more stable results and a more suitable method which can easily be used as a plug-and-play approach. In comparison with all other methods, we achieve good performance for the two most challenging datasets, namely Terra Incognita (Top-2) and DomainNet (Top-4), outperforming \rsc for both datasets by up to 2\%. This suggests, that directly reconstructing to Grad-CAMs in \divcams specifically provides value for the more challenging datasets in \domainbed. Keep in mind, that this is possible \emph{without} adding any additional parameters and while providing a framework where intermediate class activation maps can be visualized that guide the networks training process. While this might not be the \emph{best} explainability method, it certainly can offer more insights than  treating the optimization procedure as a block-box without this guidance.

\begin{table*}[t]
\footnotesize
\centering
\begin{tabular}{llcccccc}
\toprule
\textbf{Algorithm}  & \textbf{Ref.}       & \textbf{VLCS}             & \textbf{PACS}             & \textbf{Office-Home}       & \textbf{Terra Inc.}   & \textbf{DomainNet}        & \textbf{Avg.}              \\
\midrule
ERM                       & \citep{vapnik1998statistical}            & 77.5 $\pm$ 0.4            & 85.5 $\pm$ 0.2            & 66.5 $\pm$ 0.3            & 46.1 $\pm$ 1.8            & 40.9 $\pm$ 0.1            & 63.3                     \\
IRM                       & \citep{arjovsky2019invariant}             & 78.5 $\pm$ 0.5            & 83.5 $\pm$ 0.8            & 64.3 $\pm$ 2.2            & 47.6 $\pm$ 0.8            & 33.9 $\pm$ 2.8            & 61.5                      \\
GroupDRO                  & \citep{sagawa2019distributionally}        & 76.7 $\pm$ 0.6            & 84.4 $\pm$ 0.8            & 66.0 $\pm$ 0.7            & 43.2 $\pm$ 1.1            & 33.3 $\pm$ 0.2            & 60.7                      \\
Mixup                     & \citep{yan2020improve}            & 77.4 $\pm$ 0.6            & 84.6 $\pm$ 0.6            & 68.1 $\pm$ 0.3            & 47.9 $\pm$ 0.8            & 39.2 $\pm$ 0.1            & 63.4                      \\
MLDG                      & \citep{LiYSH18}            & 77.2 $\pm$ 0.4            & 84.9 $\pm$ 1.0            & 66.8 $\pm$ 0.6            & 47.7 $\pm$ 0.9            & 41.2 $\pm$ 0.1            & 63.5                      \\
CORAL                     &  \citep{SunS16}             & 78.8 $\pm$ 0.6            & 86.2 $\pm$ 0.3            & 68.7 $\pm$ 0.3            & 47.6 $\pm$ 1.0            & 41.5 $\pm$ 0.1            & 64.5                      \\
MMD                       & \citep{LiPWK18}           & 77.5 $\pm$ 0.9            & 84.6 $\pm$ 0.5            & 66.3 $\pm$ 0.1            & 42.2 $\pm$ 1.6            & 23.4 $\pm$ 9.5            & 58.8                      \\
DANN                      & \citep{GaninUAGLLML16}         & 78.6 $\pm$ 0.4            & 83.6 $\pm$ 0.4            & 65.9 $\pm$ 0.6            & 46.7 $\pm$ 0.5            & 38.3 $\pm$ 0.1            & 62.6                      \\
CDANN                     & \citep{LiTGLLZT18}          & 77.5 $\pm$ 0.1            & 82.6 $\pm$ 0.9            & 65.8 $\pm$ 1.3            & 45.8 $\pm$ 1.6            & 38.3 $\pm$ 0.3            & 62.0                      \\
MTL                       & \citep{blanchard2017domain}           & 77.2 $\pm$ 0.4            & 84.6 $\pm$ 0.5            & 66.4 $\pm$ 0.5            & 45.6 $\pm$ 1.2            & 40.6 $\pm$ 0.1            & 62.8                      \\
SagNet                    & \citep{nam2019reducing}             & 77.8 $\pm$ 0.5            & 86.3 $\pm$ 0.2            & 68.1 $\pm$ 0.1            & 48.6 $\pm$ 1.0            & 40.3 $\pm$ 0.1            & 64.2                      \\
ARM                       & \citep{zhang2020adaptive}           & 77.6 $\pm$ 0.3            & 85.1 $\pm$ 0.4            & 64.8 $\pm$ 0.3            & 45.5 $\pm$ 0.3            & 35.5 $\pm$ 0.2            & 61.7                      \\
VREx                      & \citep{krueger2020outofdistribution}            & 78.3 $\pm$ 0.2            & 84.9 $\pm$ 0.6            & 66.4 $\pm$ 0.6            & 46.4 $\pm$ 0.6            & 33.6 $\pm$ 2.9            & 61.9                      \\
RSC  		& \citep{huang2020selfchallenging}	      & 77.1 $\pm$ 0.5            & 85.2 $\pm$ 0.9            & 65.5 $\pm$ 0.9             & 46.6 $\pm$ 1.0           & 38.9 $\pm$ 0.5             & 62.7                      \\
\divcams               & (ours)            &77.8 $\pm$ 0.3            & 85.4 $\pm$ 0.2             & 65.2 $\pm$ 0.3           & 48.0 $\pm$ 1.2             & 40.7  $\pm$ 0.0    & 63.4                    \\
\dtransformers & (ours) & 78.7 $\pm$ 0.5  & 84.2 $\pm$ 0.1 & -- & 42.9 $\pm$ 1.1 & -- & -- \\
\midrule
ERM*           &   \citep{vapnik1998statistical}               	  & 77.6 $\pm$ 0.3            & 86.7 $\pm$ 0.3            & 66.4 $\pm$ 0.5            & 53.0 $\pm$ 0.3            & 41.3 $\pm$ 0.1            & 65.0                      \\
IRM*             &    \citep{arjovsky2019invariant}             	  & 76.9 $\pm$ 0.6            & 84.5 $\pm$ 1.1            & 63.0 $\pm$ 2.7            & 50.5 $\pm$ 0.7            & 28.0 $\pm$ 5.1            & 60.5                      \\
GroupDRO*      &   \citep{sagawa2019distributionally}                    & 77.4 $\pm$ 0.5            & 87.1 $\pm$ 0.1            & 66.2 $\pm$ 0.6            & 52.4 $\pm$ 0.1            & 33.4 $\pm$ 0.3            & 63.3                      \\
Mixup*               &   \citep{yan2020improve}            		 & 78.1 $\pm$ 0.3            & 86.8 $\pm$ 0.3            & 68.0 $\pm$ 0.2            & 54.4 $\pm$ 0.3            & 39.6 $\pm$ 0.1            & 65.3                      \\
MLDG*                 &  \citep{LiYSH18}           				& 77.5 $\pm$ 0.1            & 86.8 $\pm$ 0.4            & 66.6 $\pm$ 0.3            & 52.0 $\pm$ 0.1            & 41.6 $\pm$ 0.1            & 64.9                      \\
CORAL*                &  \citep{SunS16}           			  & 77.7 $\pm$ 0.2            & 87.1 $\pm$ 0.5            & 68.4 $\pm$ 0.2            & 52.8 $\pm$ 0.2            & 41.8 $\pm$ 0.1            & 65.5                      \\
MMD*               &   \citep{LiPWK18}               			& 77.9 $\pm$ 0.1            & 87.2 $\pm$ 0.1            & 66.2 $\pm$ 0.3            & 52.0 $\pm$ 0.4            & 23.5 $\pm$ 9.4            & 61.3                      \\
DANN*              &  \citep{GaninUAGLLML16}              		& 79.7 $\pm$ 0.5            & 85.2 $\pm$ 0.2            & 65.3 $\pm$ 0.8            & 50.6 $\pm$ 0.4            & 38.3 $\pm$ 0.1            & 63.8                      \\
CDANN*            &  \citep{LiTGLLZT18}              			& 79.9 $\pm$ 0.2            & 85.8 $\pm$ 0.8            & 65.3 $\pm$ 0.5            & 50.8 $\pm$ 0.6            & 38.5 $\pm$ 0.2            & 64.0                      \\
MTL*                  &  \citep{blanchard2017domain}           	  & 77.7 $\pm$ 0.5            & 86.7 $\pm$ 0.2            & 66.5 $\pm$ 0.4            & 52.2 $\pm$ 0.4            & 40.8 $\pm$ 0.1            & 64.7                      \\
SagNet*             &   \citep{nam2019reducing}           	& 77.6 $\pm$ 0.1            & 86.4 $\pm$ 0.4            & 67.5 $\pm$ 0.2            & 52.5 $\pm$ 0.4            & 40.8 $\pm$ 0.2            & 64.9                      \\
ARM*                  &   \citep{zhang2020adaptive}           		& 77.8 $\pm$ 0.3            & 85.8 $\pm$ 0.2            & 64.8 $\pm$ 0.4            & 51.2 $\pm$ 0.5            & 36.0 $\pm$ 0.2            & 63.1                      \\
VREx*                 &    \citep{krueger2020outofdistribution}       	  & 78.1 $\pm$ 0.2            & 87.2 $\pm$ 0.6            & 65.7 $\pm$ 0.3            & 51.4 $\pm$ 0.5            & 30.1 $\pm$ 3.7            & 62.5                      \\
RSC*  		& \citep{huang2020selfchallenging}	       & 77.8 $\pm$ 0.6            & 86.2 $\pm$ 0.5            & 66.5 $\pm$ 0.6            & 52.1 $\pm$ 0.2            & 38.9 $\pm$ 0.6             & 64.3                      \\
\tdivcams              & (ours) & 78.1 $\pm$ 0.6            & 87.2 $\pm$ 0.1            & 65.2 $\pm$ 0.5             & 51.3 $\pm$ 0.5           & 41.0 $\pm$ 0.0             & 64.6                      \\
\tdtransformers & (ours) & 77.7 $\pm$ 0.1 & 86.9 $\pm$ 0.3 & -- & 52.4 $\pm$ 0.8 & -- & -- \\
\bottomrule
\end{tabular}
\caption[Performance comparison across datasets]{Performance comparison across datasets using training-domain validation (top) and  oracle validation denoted with * (bottom). We use a ResNet-50 backbone, optimize with \adam, and follow the distributions specified in \domainbed. Only \rsc and our methods have been added as part of this work, the other baselines are taken from \domainbed.}
\label{tab:perfom}
\end{table*}

The fact that we are able to achieve state-of-the-art results for \divcams \emph{outside} of \domainbed shows a common problem with works in domain generalization, namely consistency in algorithm comparisons and reproducability. Due to computational constraints, novel methods are often only compared to the results provided in previous works. As a result, details such as the hyperparameter tuning procedure, learning rate schedules, or even the optimizer are often omitted or chosen to fit the algorithm at hand. From some of our experiments, we observe that simply fine-tuning a learning rate schedule for any of the methods from \Cref{tab:perfom} offers a bigger performance increase than choosing a better algorithm in the first place. As such, design choices can have a heavy impact on how well the algorithm performs. Having a common benchmarking procedure such as \domainbed, where these are fixed, is necessary to make \emph{substantial} progress in this field. We hope that we can push adoption in the community with our addition of \rsc and the methods proposed in this work. However, not using learning rate schedules and following the pre-defined distributions for learning rates, batch sizes, or weight decays might inherently bias this comparison and shouldn't be neglected as a factor.  

\begin{table*}[t]
\small
\centering
\begin{tabular}{llcccccc}
\toprule
\textbf{Algorithm} & \textbf{Ref.} & \textbf{Backbone} & \textbf{P} & \textbf{A} & \textbf{C} & \textbf{S} &  \textbf{Avg.} \\
\midrule
\textsc{Baseline}		&\cite{CarlucciDBCT19}				&	ResNet-18	&	$95.73$		&	$77.85$		&	$74.86$		&	$67.74$		&	$79.05$		 \\
\textsc{MASF}		&\cite{DouCKG19}					&	ResNet-18	&	$94.99$		&	$80.29$		&	$77.17$		&	$71.69$		&	$81.03$		 \\
\textsc{Epi-FCR}		&\cite{LiZYLSH19}					&	ResNet-18	&	$93.90$		&	$82.10$		&	$77.00$		&	$73.00$		&	$81.50$		 \\
\textsc{JiGen}		&\cite{CarlucciDBCT19}				&	ResNet-18	&	$96.03$		&	$79.42$		&	$75.25$		&	$71.35$		&	$80.51$		\\
\textsc{MetaReg}		& \cite{BalajiSC18}					&	ResNet-18	&	$95.50$		&	$83.70$		&	$77.20$		&	$70.30$		&	$81.70$		\\
\textsc{RSC} (reported)	& \cite{huang2020selfchallenging}		&	ResNet-18	&	$95.99$		&	$83.43$		&	$80.31$		&	$80.85$		&	$85.15$		\\
\textsc{RSC} (reproduced) & \cite{huang2020selfchallenging}		&	ResNet-18	&	$93.73$		&	$80.41$		&	$77.53$		&	$80.79$		&	$83.12$		\\
\divcams			&     (ours)						&	ResNet-18	&	$96.11$		&	$80.27$		&	$77.82$		&	$82.18$		&	$84.10$		\\
\bottomrule
\end{tabular}
\caption[Performance comparison for official PACS splits outside of \domainbed]{Performance comparison for PACS outside of the \domainbed framework with the official data split.}
\label{tab:official}
\end{table*}

On top of that, \Cref{tab:perfom} also shows the results for \dtransformers. Generally, we observe that many variants of the prototype based approaches outlined in \Cref{sec:prototypes} fail to generalize well to ResNet-50, even though they exhibit promising performance on ResNet-18 (see \Cref{tab:pacs_ours} for \prodrop results on PACS and ResNet-18). This might be due to the fact that prototypical approaches commonly require higher resolution feature maps \eg $14\times14$ in \citep{DoerschGZ20} via dilated convolutions. Nevertheless, \dtransformers already perform quite well for the benchmarking procedure outlined by \domainbed even without these additional changes. Keeping in mind that these adaptions can be made to push the performance even more, makes the approach even more suited for domain generalization although it is unclear how much the other algorithms would benefit from such a change. The value of prototypical approaches does not only lie in good performance but any prototypical approach can also offer a significant amount of explainability since these allow for visualizing the prototype similarity maps, the closest image patches to the prototypes, or even directly the prototypes if a sufficient decoder has jointly been trained. In particular, \dtransformers is able to achieve Top-2 performance in \domainbed for VLCS but performance seems to be not so good for the TerraIncognita dataset. We believe that this is because the dataset commonly includes only parts of the different animals at a very high distance, making it hard for the network to extract meaningful prototypes in the first place with the small feature resolution available. As such, it should be the hardest dataset for \dtransformers in \domainbed. In \Cref{tab:perfom} the results for \dtransformers on OfficeHome and DomainNet are omited due to computational constraints but we would expect the performances to be on-par if not better compared to the other methods, as these datasets do not share the same prototype extraction difficulties from TerraIncognita.


\section{Ablation Studies}
\label{sec:ablation}

In this section, we are going to look at different ablations of the presented methods and how the individual components impact the performance. In particular, for \divcams, we analyze different methods of resetting the masks (mask batching) in \Cref{sec:ablation_study_batching} and see how additional methods that are supposed to improve the underlying class activation maps impact performance in \Cref{sec:abl-masks}. On top of that, for \prodrop, we evaluate the effect of self-challenging for different negative weights in \Cref{sec:abl_self_challenging}, as well as the impact of the additional intra-loss factor in \Cref{sec:intra_loss}.

\subsection{Hyperparameter Distributions \& Schedules}
\label{sec:abl-distr}
For the mask batching ablation study we use \adam \cite{Kingma2015} and the distributions from \Cref{tab:abl-distributions-mask-batching}. When the batch drop factor is scheduled, we use an increasing linear schedule while the learning rate is always scheduled with a step-decay which decays the learning rate by factor $0.1$ at epoch $80/100$.
\begin{table}[t]
    \centering
    \begin{tabular}{lll}
        \toprule
         & \textbf{Hyperparameter} & \textbf{Distribution} \\
        \midrule
        $\alpha$ & learning rate & $\loguni{-5}{-1}$ \\
        $\mathcal{B}$ & batch size  & $\floor{\logunitwo{3}{9}}$ \\
        $\gamma$ & weight decay  & $\loguni{-6}{-2}$ \\
        $p$ & feature drop factor  & $1/3$ \\
        $b$ & batch drop factor  & $\uni{0}{1}$ \\
         \bottomrule 
    \end{tabular}
    \caption[Hyperparameters and distributions used for the mask batching ablation study]{Hyperparameters and distributions used in random search for the mask batching ablation study. $\logunix{a}{b}$ denotes a log-uniform distribution between $a$ and $b$ for base $x$, the uniform distribution is denoted as $\uni{a}{b}$ and $\floor{\cdot}$ is the floor operator.}
    \label{tab:abl-distributions-mask-batching}
\end{table}

For the mask ablation study, we use \adam \cite{Kingma2015} and the distributions from \Cref{tab:abl-distributions-mask}. When the batch drop factor is scheduled, we use an increasing linear schedule while the learning rate is \emph{not} scheduled. This corresponds to the tuning distributions provided in \domainbed which are also used for all the main results and all other ablations. If not marked otherwise, each experiment evaluates $20$ hyperparameter samples, similar to what is suggested in \domainbed.

\begin{table}[t]
\small
    \centering
    \begin{tabular}{lll}
        \toprule
        & \textbf{Hyperparameter} & \textbf{Distribution} \\
        \midrule
        $\alpha$ & learning rate & $\loguni{-5}{-3.5}$ \\
        $\mathcal{B}$ & batch size  & $\floor{\logunitwo{3}{5.5}}$ \\
        $\gamma$ & weight decay  & $\loguni{-6}{-2}$ \\
        $p$ & feature drop factor  & $\uni{0.2}{0.5}$ \\
        $b$ & batch drop factor  & $\uni{0}{1}$ \\
        $\lambda_1$ & hnc factor & $\loguni{-3}{-1}$ \\
        $k$ & negative classes & $\mathrm{num\_classes} - 1$ \\
        $\lambda_{tap}$ & tap factor & $\uni{0}{1}$ \\
        $\lambda_2$ & adversarial factor & $\loguni{-2}{2}$ \\
        $s$ &disc per gen step & $\floor{\logunitwo{0}{3}}$ \\
        $\eta$ & gradient penalty & $\loguni{-2}{1}$ \\
        $\omega_s$ & mlp width & $512$ \\
        $\omega_d$ & mlp depth & $3$ \\
        $\omega_{dr}$ & mlp dropout & $0.5$ \\
        $\lambda_3$ & mmd factor & $\loguni{-1}{1}$ \\
        \bottomrule 
    \end{tabular}
    \caption[Hyperparameters and distributions used for the mask ablation study]{Hyperparameters and distributions used in random search for the mask ablation study. $\logunix{a}{b}$ denotes a log-uniform distribution between $a$ and $b$ for base $x$, the uniform distribution is denoted as $\uni{a}{b}$, $\floor{\cdot}$ is the floor operator.}
    \label{tab:abl-distributions-mask}
\end{table}

\subsection{\divcam: Mask Batching}
\label{sec:ablation_study_batching}

There exist several methods how we can compute the vector $\mathbf{c}$ in our method and hence determine how we should apply the masks within each batch. Here, we analyze the effect on performance for a few possible choices. By default, \divcam uses \Cref{eq:conf_scamb} where $y_{gt}$ is the confidence on the ground truth class after softmax. This applies the masks on samples with the highest confidence on the correct class within each batch.   
\begin{equation}
\label{eq:conf_scamb}
	\mathbf{c}^n = y_{gt}
\end{equation}
Another option is \divcamc with \Cref{eq:conf_scamc} which computes the change in confidence on the ground truth class when applying the mask. The masked confidence after softmax is denoted as $\tilde{y}_{gt}$. This variation applies the masks for samples where the mask decreases confidence on the ground truth class the most.
\begin{equation}
\label{eq:conf_scamc}
   \mathbf{c}^n = y_{gt} - \tilde{y}_{gt}
\end{equation}
The last variation is \divcamt where we apply the masks randomly for samples which are correctly classified. All variants can further be extended by adding a linear schedule, denoted with an additional ``S'', or computing $\mathbf{c}$ for each domain separately, denoted with an additional ``D''. By adding a schedule, we apply masks more in the later training epochs where discriminant features have been learned and by enforcing it for each domain we can ensure that the masks aren't applied biased towards a subset of domains and disregarded for others. \Cref{tab:scam_batching} shows experiments for these variants.


\begin{table*}[t]
\small
%\renewcommand{\arraystretch}{1.2}
    \centering
    \begin{tabular}{lccccc}
    \toprule
    \textbf{Name} &  \textbf{P} & \textbf{A} & \textbf{C} & \textbf{S} &  \textbf{Avg.} \\
    \midrule
    %\scam & $84.4\pm1.1$ & $74.2\pm2.3$ & $72.8\pm1.7$ & $63.0\pm6.8$ & $73.6\pm2.2$  \\
    \divcam & $94.0\pm0.4$ & $80.6\pm1.2$ & $75.4\pm0.7$ & $76.7\pm0.7$ & $81.7\pm0.6$ \\
    \divcams & $94.4\pm0.7$ & $80.5\pm0.4$ & $74.6\pm2.2$ & $\tabtop{79.0\pm0.9}$ & $\tabtop{82.1\pm0.3}$   \\
    \divcamd & $94.3\pm0.1$ & $80.1\pm0.1$ & $74.5\pm0.9$ & $76.6\pm1.7$ & $81.4\pm0.2$   \\
    \divcamds & $93.9\pm0.2$ & $80.4\pm0.4$ & $73.4\pm2.2$ & $74.8\pm1.2$ & $80.6\pm0.9$   \\
    \divcamc & $92.6\pm0.4$ & $80.1\pm1.1$ & $73.6\pm1.4$ & $75.0\pm1.2$ & $80.3\pm0.9$  \\
    \divcamcs & $95.0\pm0.6$ & $79.9\pm1.0$ & $74.5\pm0.7$ & $78.1\pm0.8$ & $81.9\pm0.4$   \\
    \divcamdc & $\tabtop{95.1\pm0.4}$ & $79.5\pm1.0$ & $73.7\pm0.9$ & $75.2\pm1.2$ & $80.9\pm0.4$ \\
    \divcamdcs & $93.5\pm0.1$ & $80.1\pm0.2$ & $75.1\pm0.1$ & $77.2\pm1.6$ & $81.5\pm0.5$  \\
    \divcamt & $95.0\pm0.3$ & $80.3\pm0.3$ & $74.8\pm0.8$ & $75.3\pm1.1$ & $81.4\pm0.4$  \\
    \divcamts & $95.0\pm0.1$ & $79.9\pm0.8$ & $72.6\pm1.3$ & $77.1\pm1.4$ & $81.2 \pm 0.4$  \\
    \divcamdt & $94.8\pm0.6$ & $79.6\pm0.6$ & $74.0\pm1.1$ & $78.5\pm0.4$ & $81.7\pm0.1$  \\
    \divcamdts & $95.1\pm0.2$ & $\tabtop{81.5\pm1.3}$ & $\tabtop{75.5\pm0.4}$ & $74.9\pm2.0$ &  $81.7\pm0.5$  \\
    \midrule
    %\tscam & $84.1\pm0.7$ & $72.9\pm2.7$ & $73.9\pm1.1$ & $68.2\pm4.3$ & $74.8\pm1.3$   \\
    \tdivcam & $\tabtop{94.9\pm0.7}$ & $81.5\pm0.7$ & $76.6\pm0.4$ & $\tabtop{80.5\pm0.7}$ & $83.4\pm0.3$  \\
    \tdivcams & $94.9\pm0.3$ & $\tabtop{82.7\pm0.7}$ & $76.3\pm0.7$ & $80.1\pm0.4$ & $83.5\pm0.3$   \\
    \tdivcamd & $94.8\pm0.2$ & $81.0\pm0.7$ & $\tabtop{77.6\pm0.6}$ & $79.9\pm0.6$ &  $83.3\pm0.3$  \\
    \tdivcamds & $94.6\pm0.5$ & $80.7\pm0.3$ & $77.0\pm0.4$ & $79.3\pm0.3$ & $82.9\pm0.1$   \\
    \tdivcamc & $94.7\pm0.5$ & $82.6\pm0.6$ & $77.0\pm0.5$ & $80.1\pm1.0$ & $\tabtop{83.6\pm0.3}$  \\
    \tdivcamcs & $94.2\pm0.2$ & $82.5\pm0.8$ & $76.9\pm0.3$ & $79.9\pm0.7$ & $83.4\pm0.3$  \\
    \tdivcamdc & $94.8\pm0.4$ & $82.0\pm0.4$ & $76.6\pm0.9$ & $80.1\pm0.4$ & $83.4\pm0.1$  \\
    \tdivcamdcs & $94.7\pm0.4$ & $81.0\pm0.3$ & $77.6\pm0.2$ & $80.3\pm1.3$ & $83.4\pm0.3$ \\
    \tdivcamt & $94.5\pm0.4$ & $81.6\pm0.8$ & $76.7\pm0.2$ & $79.6\pm0.4$ & $83.1\pm0.4$ \\
    \tdivcamts & $94.8\pm0.3$ & $81.3\pm0.2$ & $76.7\pm0.5$ & $79.7\pm0.5$  & $83.2 \pm 0.2$  \\
    \tdivcamdt & $94.7\pm0.5$ & $80.9\pm1.1$ & $77.3\pm0.5$ & $79.9\pm0.6$ & $83.2\pm0.2$ \\
    \tdivcamdts & $94.7\pm0.5$ & $82.1\pm1.0$ & $76.4\pm0.6$ & $79.5\pm1.2$ & $83.2\pm0.1$  \\
    \bottomrule
    \end{tabular}
    \caption[Ablation study for the \divcam mask batching on the PACS dataset]{Ablation study for the \divcam mask batching on the PACS dataset using training-domain validation (top) and oracle validation denoted with * (bottom). We use a ResNet-18 backbone, schedules and distributions from \Cref{sec:abl-distr}, $25$ hyperparameter samples, and $3$ split seeds for standard deviations.}
    \label{tab:scam_batching}
\end{table*}

We observe that adding a schedule helps in most cases, achieving the highest training domain validation performance for \divcams.  Enforcing the application of masks within each domain, however, doesn't consistently improve performance and therefore we don't consider it for the final method.

\subsection{\divcam: Class Activation Maps}
\label{sec:abl-masks}

We combine our class activation maps with other methods from domain generalization, as well as methods to boost the explainability for class activation maps from the weakly-supervised object localization literature. The results are shown in \Cref{tab:scam_masks} where MAP + CDANN drops the self-challenging part from \divcam and just computes ordinary cross entropy while aligning the class activation maps.

We observe that, surprisingly, none of the methods have a positive effect for training domain validation even though some of them exhibit better performance for oracle validation. Notably, especially the CDANN approach tends to exhibit a high 
standard deviation for some of the domains (\eg art) which suggests that this approach can be fine-tuned when only reporting performance on a single seed. However, since we are looking for a method which reliably provides competitive results, regardless of the used seed and without needing extensive fine-tuning, we disregard this option.
\begin{table}[t]
\small
%\renewcommand{\arraystretch}{1.2}
    \centering
    \begin{tabular}{lccccc}
    \toprule
    \textbf{Name} &  \textbf{P} & \textbf{A} & \textbf{C} & \textbf{S} & \textbf{Avg.} \\
    \midrule 
    \divcam & $\tabtop{97.6\pm0.4}$ & $85.2\pm0.8$ & $80.5\pm0.7$ & $78.3\pm0.8$ & $\tabtop{85.4\pm0.5}$ \\
    \divcams & $97.3\pm0.4$ & $86.2\pm1.4$ & $79.1\pm2.2$ & $\tabtop{79.2\pm0.1}$ & $85.4\pm0.2$ \\
    \divcams + TAP & $96.9\pm0.1$ & $85.1\pm1.5$ & $78.7\pm0.4$ & $75.3\pm0.6$ & $84.0\pm0.4$ \\
    \divcams + HNC & $97.2\pm0.3$ & $\tabtop{87.2\pm0.9}$ & $79.2\pm0.6$ & $71.7\pm3.1$ & $83.8\pm0.4$ \\
    \divcams + CDANN & $97.5\pm0.4$ & $85.2\pm2.8$ & $78.3\pm2.0$ & $74.8\pm0.9$ & $84.0\pm1.5$ \\
    \divcams + MMD & $97.0\pm0.2$ & $85.4\pm1.0$ & $\tabtop{81.5\pm0.4}$ & $75.8\pm3.5$ & $84.9\pm1.1$ \\
    CAM + CDANN & $97.2\pm0.3$ & $86.7\pm0.5$ & $77.3\pm1.7$ & $71.5\pm1.3$ & $83.2\pm0.8$ \\
    \midrule
    \tdivcam & $96.2\pm1.2$ & $87.0\pm0.5$ & $82.0\pm0.9$ & $80.8\pm0.6$ & $86.5\pm0.1$ \\
    \tdivcams & $97.2\pm0.3$ & $86.5\pm0.4$ & $83.0\pm0.5$ & $82.2\pm0.1$ & $87.2\pm0.1$ \\
    \divcams + TAP* & $97.3\pm0.3$ & $87.2\pm0.8$ & $\tabtop{83.2\pm0.8}$ & $\tabtop{82.8\pm0.2}$ & $\tabtop{87.6\pm0.0}$ \\
    \divcams + HNC*  & $97.3\pm0.2$ & $\tabtop{87.4\pm0.5}$ & $81.4\pm0.6$ & $79.7\pm1.1$ & $86.5\pm0.4$ \\
    \divcams + CDANN* & $\tabtop{97.3\pm0.5}$ & $85.9\pm1.2$ & $80.6\pm0.4$ & $80.9\pm0.4$ & $86.2\pm0.2$ \\
    \divcams + MMD* & $\tabtop{97.3\pm0.5}$ & $86.8\pm0.7$ & $83.2\pm0.4$ & $80.9\pm0.7$ & $87.1\pm0.4$ \\
    CAM + CDANN* & $97.2\pm0.4$ & $86.7\pm0.5$ & $81.9\pm0.2$ & $80.6\pm0.7$ & $86.6\pm0.2$ \\
    \bottomrule
    \end{tabular}
    \caption[Ablation study for the \divcam masks on the PACS dataset]{Ablation study for the \divcam masks on the PACS dataset using training-domain validation (top) and oracle validation denoted with * (bottom). We use a ResNet-50 backbone, schedules and distributions from \Cref{sec:abl-distr}, $20$ hyperparameter samples, and $3$ split seeds for standard deviations. Results are directly integratable in \Cref{tab:perfom} as we use the same tuning protocol provided in \domainbed.}
    \label{tab:scam_masks}
\end{table}

\subsection{\prodrop: Self-Challenging}
\label{sec:abl_self_challenging}

\Cref{tab:scabl} shows the ablation results for the self-challenging addition. We observe that for most negative weights, adding self-challenging results in an performance increase. Most notably, this occurs for cases where the performance without self-challenging is very poor such as $w_{c,j} = -0.2\; \forall j: \prot \notin \prots_c$,  $w_{c,j} = -1.0\; \forall j: \prot \notin \prots_c$, or $w_{c,j} = 0.0\; \forall j: \prot \notin \prots_c$. Generally, in cases where it does not lead to a performance increase, the downside seems to be very small where we at most drop by 0.5\% performance for $w_{c,j} = -0.5\; \forall j: \prot \notin \prots_c$. 

If we look at the performance changes solely based on the different negative weights, we can't observe any consistent trends. In fact, it is very surprising that small changes such as from  $w_{c,j} = -0.1$ to $w_{c,j} = -0.2$ without self-challenging can lead to a 2\% performance change. 

\begin{table}[t]
    \centering
    \begin{tabular}{lcccccc}
    \toprule
    \textbf{Weight}  & \textbf{SC} & \textbf{P} & \textbf{A} & \textbf{C} & \textbf{S} & \textbf{Avg.} \\
     \midrule
     \phantom{-}0.0 & \ding{55} & 93.2 $\pm$ 0.0 & 80.4 $\pm$ 1.0 & 73.7 $\pm$ 0.4 & 72.6 $\pm$ 2.6 & 80.0 $\pm$ 0.7 \\
     -0.1 & \ding{55} & 94.3 $\pm$ 0.3 & 78.8 $\pm$ 0.3 & 74.4 $\pm$ 0.7 & 75.3 $\pm$ 1.6 & 80.7 $\pm$ 0.4 \\
     -0.2 & \ding{55} & 93.2 $\pm$ 0.5 & 76.8 $\pm$ 1.5 & 72.9 $\pm$ 0.1 & 71.8 $\pm$ 1.0 & 78.7 $\pm$ 0.5 \\
     -0.3 & \ding{55} & 94.0 $\pm$ 0.4 & 79.8 $\pm$ 1.0 & 75.6 $\pm$ 1.5 & 73.9 $\pm$ 1.0 & 80.8 $\pm$ 0.1 \\
     -0.4 & \ding{55} & 93.6 $\pm$ 0.1 & 79.8 $\pm$ 0.5 & 74.3 $\pm$ 1.3 & 75.7 $\pm$ 2.3 & 80.8 $\pm$ 0.5 \\
     -0.5 & \ding{55} & 93.0 $\pm$ 0.9 & 79.4 $\pm$ 1.6 & 73.2 $\pm$ 0.9 & 75.5 $\pm$ 1.0 & 80.3 $\pm$ 0.4 \\
     -1.0 & \ding{55} & 94.2 $\pm$ 0.4 & 80.2 $\pm$ 1.1 & 72.7 $\pm$ 1.4 & 68.6 $\pm$ 0.6 & 78.9 $\pm$ 0.2 \\
     -2.0 & \ding{55} & $\tabtop{94.7 \pm 0.4}$ & 78.7 $\pm$ 0.5 & 75.5 $\pm$ 1.0 & 71.1 $\pm$ 2.2 & 80.0 $\pm$ 0.7 \\
     \phantom{-}0.0 $\to$ -1.0 & \ding{55} & 94.3 $\pm$ 0.5 & 79.5 $\pm$ 0.6 & 74.2 $\pm$ 0.3 & 72.2 $\pm$ 2.1 & 80.0 $\pm$ 0.4 \\
      \midrule
      \phantom{-}0.0 & \ding{51} & 93.4 $\pm$ 0.6 & 80.5 $\pm$ 0.8 & 75.6 $\pm$ 0.1 & 74.3 $\pm$ 2.0 & 81.0 $\pm$ 0.4 \\
     -0.1 & \ding{51} & 93.7 $\pm$ 0.3 & $\tabtop{83.2 \pm 1.4}$ & 75.9 $\pm$ 1.2 & 71.1 $\pm$ 1.9 & 81.0 $\pm$ 0.8 \\
     -0.2 & \ding{51} & 93.2 $\pm$ 0.2 & 81.0 $\pm$ 0.7 & 73.9 $\pm$ 0.7 & 75.0 $\pm$ 0.6 & 80.8 $\pm$ 0.1 \\
     -0.3 & \ding{51} & 93.4 $\pm$ 0.8 & 81.4 $\pm$ 0.4 & 71.3 $\pm$ 1.2 & 76.9 $\pm$ 0.9 & 80.7 $\pm$ 0.2 \\
     -0.4 & \ding{51} & 94.0 $\pm$ 0.3 & 81.7 $\pm$ 0.9 & 72.9 $\pm$ 0.4 & 73.5 $\pm$ 1.0 & 80.5 $\pm$ 0.2 \\
     -0.5 & \ding{51} & 93.5 $\pm$ 0.7 & 80.7 $\pm$ 1.6 & 71.6 $\pm$ 1.4 & 73.6 $\pm$ 1.5 & 79.8 $\pm$ 0.9 \\
     -1.0 & \ding{51} & 94.6 $\pm$ 0.2 & 81.6 $\pm$ 1.2 & 72.9 $\pm$ 0.4 & $\tabtop{77.0 \pm 1.6}$ & $\tabtop{81.5 \pm 0.2}$ \\
     -2.0 & \ding{51} & 94.0 $\pm$ 0.5 & 79.5 $\pm$ 1.2  & $\tabtop{76.4 \pm 0.4 }$ & 73.9 $\pm$ 1.7  & 80.9 $\pm$ 0.6 \\
     \phantom{-}0.0 $\to$ -1.0 & \ding{51} & 94.1 $\pm$ 0.4 & 79.2 $\pm$ 0.6 & 74.1 $\pm$ 1.1 & 71.9 $\pm$ 0.2 & 79.8 $\pm$ 0.3  \\
    \bottomrule
    \end{tabular}
    \caption[Self-challenging performance comparison for different negative class weights]{Performance comparison for different negative class weights on the PACS dataset without (top) and with self-challenging (bottom)  using training-domain validation and a ResNet-18 backbone. The feature and batch drop factors are kept constant with $p=0.5$ and $b=\frac{1}{3}$, a linear schedule from $a$ to $b$ throughout training is denoted with $a \to b$.}
    \label{tab:scabl}
\end{table}




\subsection{\prodrop: Intra-Loss}
\label{sec:intra_loss}

\Cref{tab:intrabl} shows the ablation results for different intra factor strengths $\lambda_6$ with and without self-challenging. As expected, if the intra factor grows too large, performance degrades. For smaller values of $\lambda_6$ without self-challenging, we can observe a consistent performance increase of varying degrees similar to what self-challenging is able to gain.

Even though we experimented with different weighting of the individual distance metrics as well as the overall loss, we observe no \emph{consistent} performance improvements on top of self-challenging across testing environments and data splits. The observations from \Cref{sec:intra_loss}, \Cref{fig:pw_distance_trial1-sc}, and \Cref{sec:additional_distances} suggest that self-challenging inherently already enforces the desired properties up to the near-optimal extent such that the additional loss term does not provide any consistent further benefits.

\begin{table}[ht]
    \centering
    \begin{tabular}{lcccccc}
    \toprule
    Intra factor \textbf{$\lambda_6$}  & \textbf{SC} & \textbf{P} & \textbf{A} & \textbf{C} & \textbf{S} & \textbf{Avg.} \\
     \midrule
     \phantom{-}0.0 & \ding{55} & 94.2 $\pm$ 0.4 & 80.2 $\pm$ 1.1 & 72.7 $\pm$ 1.4 & 68.6 $\pm$ 0.6 & 78.9 $\pm$ 0.2 \\
     -0.1 & \ding{55} & 94.1 $\pm$ 0.4 & 81.2 $\pm$ 1.2 & 73.6 $\pm$ 0.8 & 75.2 $\pm$ 2.4 & 81.0 $\pm$ 0.6 \\
     -0.2 & \ding{55} & 94.5 $\pm$ 0.4 & 81.5 $\pm$ 1.1 & 74.4 $\pm$ 1.6 & 74.4 $\pm$ 1.1 & 81.2 $\pm$ 0.5 \\
     -0.5 & \ding{55} & 92.9 $\pm$ 0.8 & 82.4 $\pm$ 0.4 & 73.5 $\pm$ 1.6 & 73.4 $\pm$ 2.0 & 80.6 $\pm$ 0.7 \\
     -1.0 & \ding{55} & 94.7 $\pm$ 0.4 & 80.4 $\pm$ 0.6 & 73.9 $\pm$ 0.9 & 75.2 $\pm$ 1.8 & 81.1 $\pm$ 0.7 \\
      \midrule
      \phantom{-}0.0 & \ding{51} & 94.6 $\pm$ 0.2 & 81.6 $\pm$ 1.2 & 72.9 $\pm$ 0.4 & $\tabtop{77.0 \pm 1.6}$ & $81.5 \pm 0.2$ \\
      -0.1 & \ding{51} & 94.6 $\pm$ 0.3 & 82.6 $\pm$ 0.9 & 72.2 $\pm$ 0.5 & 75.4 $\pm$ 0.1 & 81.2 $\pm$ 0.3 \\
      -0.2 & \ding{51} & 94.1 $\pm$ 0.1 & 81.9 $\pm$ 0.2 & 73.3 $\pm$ 0.7 & 75.8 $\pm$ 0.9 & 81.2 $\pm$ 0.1 \\
      -0.3 & \ding{51} & 94.8 $\pm$ 0.2 & 81.5 $\pm$ 0.1 & 75.1 $\pm$ 0.0 & 74.5 $\pm$ 0.4 & 81.5 $\pm$ 0.2 \\
      -0.4 & \ding{51} & 93.9 $\pm$ 0.5 & 82.4 $\pm$ 0.2 & 74.3 $\pm$ 1.3 & 76.1 $\pm$ 1.1 & 81.7 $\pm$ 0.2 \\
      -0.5 & \ding{51} & 93.6 $\pm$ 0.6 & 82.1 $\pm$ 0.9 & $\tabtop{76.4 \pm 0.9}$ & 76.3 $\pm$ 0.6 & $\tabtop{82.1 \pm 0.6}$ \\
      -0.6 & \ding{51} & 93.8 $\pm$ 0.6 & 82.1 $\pm$ 0.3 & 75.7 $\pm$ 0.2 & 73.1 $\pm$ 3.1 & 81.2 $\pm$ 1.0 \\
      -0.7 & \ding{51} & 94.0 $\pm$ 0.4 & $\tabtop{82.9 \pm 1.2}$ & 74.1 $\pm$ 0.5 & 76.3 $\pm$ 1.0 & $81.8 \pm 0.4$ \\
      -0.8 & \ding{51} & 94.1 $\pm$ 0.5 & 81.8 $\pm$ 0.3 & 75.5 $\pm$ 0.2 & 72.7 $\pm$ 0.3 & $81.0 \pm 0.1$ \\
      -1.0 & \ding{51} & $\tabtop{95.1 \pm 0.6}$ & 80.7 $\pm$ 1.5 & 74.5 $\pm$ 1.1 & 73.7 $\pm$ 1.2 & 81.0 $\pm$ 0.4 \\
      -2.0 & \ding{51} & 94.3 $\pm$ 0.3 & $\tabtop{82.9 \pm 0.5}$ & 75.1 $\pm$ 1.0 & 75.0 $\pm$ 1.1 & 81.8 $\pm$ 0.5 \\
      -3.0 & \ding{51} & 94.5 $\pm$ 0.3 & 79.8 $\pm$ 1.0 & 74.9 $\pm$ 1.0 & 71.2 $\pm$ 0.4 & 80.1 $\pm$ 0.3 \\
      -10.0 & \ding{51} & 90.8 $\pm$ 3.7 & 80.2 $\pm$ 0.3 & 72.9 $\pm$ 0.1 & 75.0 $\pm$ 1.8 & 79.7 $\pm$ 0.6 \\
      -100.0 & \ding{51} & 72.0 $\pm$ 11. & 76.3 $\pm$ 2.2 & 73.0 $\pm$ 1.1 & 69.1 $\pm$ 2.6 & 72.6 $\pm$ 4.0 \\
    \bottomrule
    \end{tabular}
    \caption[Self-challenging performance comparison for different intra factors]{Performance comparison for different intra factors on the PACS dataset without (top) and with self-challenging (bottom)  using training-domain validation and a ResNet-18 backbone. The feature and batch drop factors are kept constant with $p=0.5$ and $b=\frac{1}{3}$, negative weight is $-1.0$. Distance metrics are weighted with $\lambda_{\ell_2} = 1$ and $\lambda_\cdistance = 1$.}
    \label{tab:intrabl}
\end{table}