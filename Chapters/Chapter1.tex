\chapter{Introduction} 
\label{sec:Introduction}

Modern machine learning solutions commonly rely on supervised deep neural networks as their default approach and one of the tasks that is commonly required and implemented in practice is \emph{image classification}. In its simplest form, the used networks combine multiple layers of linear transformations coupled with non-linear activation functions to \emph{classify} an input image based on its pixel values into a discrete set of classes. The chosen architecture, also known as the model, is then able to \emph{learn} good parameters  that represent how to combine the information extracted from the individual pixel values using additional labeled information. That is, for a set of images we know the correct class and can automatically guide the network towards well-working parameters by determining how wrong the current prediction is and in which direction we need to update the individual parameters, for our network to give a more accurate class prediction. 

Obtaining this labeled information, however, is very tedious in practice and either requires a lot of manual human labeling or sufficient human quality assurance for any automatic labeling system. A commonly used approach to overcome this impediment is to combine multiple sources of data, that might have been collected in different settings but represent the same set of classes and have already been labeled a priori. Since the training distribution described by the obtained labeled data is then often different from the testing distribution imposed by the images we observe once we deploy our system, we commonly observe a \emph{distribution shift} when testing and the network generally needs to do \emph{out-of-distribution} predictions. Similar behavior can be observed when the model encounters irregular properties during testing such as weather or lighting conditions which have not been captured well by the training data. For many computer vision neural network models, this poses an interesting and challenging task which is known as \emph{out-of-distribution generalization} where researchers try to improve the predictions under these alternating circumstances for more robust machine learning models.

In this work, we pick up on this challenge and try to improve the out-of-distribution generalization capabilities with models and techniques that have been proposed to make deep neural networks more explainable. For most machine learning settings, gaining a degree of explainability that tries to give humans more insights into \emph{how} and \emph{why} the network arrives at its predictions, restricts the underlying model and hinders performance to a certain degree. For example, decision trees are thought of as being more explainable than deep neural networks but lack performance on visual tasks. In this work, we investigate if these properties also hold for the out-of-distribution generalization task or if we can deploy explainability methods during the training procedure and gain, both, better performance as well as a framework that enables more explainability for the users. In particular, we develop a regularization technique based on class activation maps that visualize parts of an image that led to certain predictions (\divcam) as well as prototypical representations that serve as a number of class or attribute centroids which the network uses to make its predictions (\prodrop and \dtransformers). Specifically, we deploy these methods for the \emph{domain generalization} task where the model has access to images from multiple training domains, each imposing a different distribution, but without access to images from the immediate testing distribution.

From our experiments, we observe that especially \divcam offers state-of-the-art performance on some datasets while providing a framework that enables additional insights into the training and prediction procedure. For us, this is a property that is highly desirable, especially in safety-critical scenarios such as self-driving cars, any application in the medical field such as cancer or tumor prediction, or any other automation robot that needs to operate in a diverse set of environments. Hopefully, some of the methods presented in this work can find application in such scenarios and establish additional trust and confidence into the machine learning system to work reliable. All of our experiments have been conducted within the \domainbed domain generalization benchmarking framework and the respective code has been open-sourced.\footnote{All implementation details can be found here: \url{https://github.com/SirRob1997/DomainBed/}}  












