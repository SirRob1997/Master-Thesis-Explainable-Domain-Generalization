\chapter{Conclusion and Outlook}

In this work, we investigated if we can deploy explainability methods during the training procedure and gain, both, better performance on the domain generalization task as well as a framework that enables more explainability for the users. In particular, we develop a regularization technique based on class activation maps that visualize parts of an image responsible for certain predictions (\divcam) as well as prototypical representations that serve as a number of class or attribute centroids which the network uses to make its predictions (\prodrop and \dtransformers).

From the results and ablations presented in this work, we have shown that especially \divcam is a reliable method for achieving domain generalization for small to medium sized ResNet backbones while offering an architecture that allows for additional insights into \emph{how} and \emph{why} the network arrives at it's predictions. Depending on the backbone and dataset, \prodrop and \dtransformers can also be powerful options for this cause. The possibilities for explainability in these methods is a property that is highly desirable in practice, especially for safety-critical scenarios such as self-driving cars, any application in the medical field \eg cancer or tumor prediction, or any other automation robot that needs to operate in a diverse set of environments. We hope that the presented methods can find application in such scenarios and establish additional trust and confidence into the machine learning systems to work reliable.

Building upon the methods presented in this work, there are also quite a number of ablations or extension points that might be interesting to investigate. First, even though the results for \prodrop looked very promising on ResNet-18, it failed to generalize well enough to ResNet-50 to properly compete with the other methods in \domainbed. Looking into some of the details coming from this transition, might yield a better understanding of what causes this problem and a solution can probably be found. Secondly, even though the explainability methods we build upon have very deep roots in the explainability literature, it would be interesting to either jointly train a suitable decoder for the prototypes or visualize the closest latent patch across the whole dataset. Since we're training with images coming from different domains, there could be interesting visualizations possible, potentially also in a video format that shows the change throughout training. In this work, we especially focused on achieving good performance with these methods rather than the explainability itself. Thirdly, many prototypical networks upscale the feature map in size, for example to $14 \times 14$ from $7 \times 7$, and report great performance gains coming from the increased spatial resolution of the latent representation \citep{DoerschGZ20}. We deliberately refrain from changing the backbone in such a way to improve comparability in the \domainbed framework without needing to re-compute the extensive set of baselines, although it might be possible that both \prodrop and \dtransformers benefit more heavily from such a change compared to other methods. In a similar fashion, many prototypical networks use the euclidean distance as a distance measure while some works report better performance for the dot or cosine similarity \citep{XuXWSA20}. We experimented with a few options across the different methods but a clear ablation study of this detail for the domain generalization task would be very helpful. In particular, one can also think about deploying any other Bregman divergence measure and taking a metric learning approach with, for example, the Mahalanobis distance \citep{BanerjeeMDG04} that might be able to achieve additional performance gains. 

Finally, especially for \dtransformers, our method of aggregating across multiple environments is very simple in nature and finding a more elaborate way to utilize the multiple aligned prototypes for each environment might be a great opportunity for a follow-up work. Nevertheless, we hope that our analysis might serve as a first stepping stone and groundwork for developing more domain generalization algorithms based on explainability methods. 