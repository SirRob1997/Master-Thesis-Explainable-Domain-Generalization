% Appendix A

\chapter{Ablation Study: Details} % Main appendix title
\label{sec:abl-distr} % For referencing this appendix elsewhere, use \ref{AppendixA}
All ablation studies were performed using \domainbed \citep{gulrajani2020search}. We experimented with distributions, optimizers, learning rate schedules and epochs to determine the effect on the performance. The results of our final method, however, are based on the same tuning protocol as proposed in \domainbed to simplify the comparison.

For the mask batching ablation study we use \adam \citep{Kingma2015} and the distributions from \Cref{tab:abl-distributions-mask-batching}. When the batch drop factor is scheduled, we use an increasing linear schedule while the learning rate is always scheduled with a step-decay which decays the learning rate by factor $0.1$ at epoch $80/100$.
\begin{table}[!htbp]
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Hyperparameter} & \textbf{Distribution} \\
        \midrule
        learning rate & $\loguni{-5}{-1}$ \\
        batch size  & $\floor*{\logunitwo{3}{9}}$ \\
        weight decay  & $\loguni{-6}{-2}$ \\
        feature drop factor  & $1/3$ \\
        batch drop factor  & $\uni{0}{1}$ \\
         \bottomrule 
    \end{tabular}
    \caption[Hyperparameters and distributions used for the mask batching ablation study]{Hyperparameters and distributions used in random search for the mask batching ablation study. $\logunix{a}{b}$ denotes a log-uniform distribution between $a$ and $b$ for base $x$, the uniform distribution is denoted as $\uni{a}{b}$ and $\floor*{\cdot}$ is the floor operator.}
    \label{tab:abl-distributions-mask-batching}
\end{table}

% \begin{table}[!htbp]
%     \centering
%     \begin{tabular}{lcccc}
%         \toprule
%         \textbf{Hyperparameter} & \textbf{P} & \textbf{A} & \textbf{C} & \textbf{S}\\
%         \midrule
%         learning rate & $(1.18e-5, 5.61e-5, 2.75e-5)$ & & & \\
%         batch size  & $(12, 15, 38)$ & & &\\
%         weight decay  & $(4.38e-5, 1.20e-6, 1,08e-05)$ & & & \\
%         feature drop factor  & $(1/3, 1/3, 1/3)$ & & & \\
%         batch drop factor  & $(0.44, 0.93, 0.48)$ & & & \\
%         \bottomrule 
%     \end{tabular}
%     \caption[Selected hyperparameters in random search for the mask batching ablation study]{Selected hyperparameters in random search for the mask batching ablation study}
%     \label{tab:abl-distributions-mask-batching-params}
% \end{table}

For the mask ablation study, we use \adam \citep{Kingma2015} and the distributions from \Cref{tab:abl-distributions-mask}. When the batch drop factor is scheduled, we use an increasing linear schedule while the learning rate is \emph{not} scheduled. This corresponds to the tuning distributions provided in \domainbed.
\begin{table}[!htbp]
\small
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Hyperparameter} & \textbf{Distribution} \\
        \midrule
        learning rate & $\loguni{-4.5}{-2.5}$ \\
        batch size  & $\floor*{\logunitwo{3}{9}}$ \\
        weight decay  & $\loguni{-6}{-2}$ \\
        feature drop factor  & $\uni{0}{0.5}$ \\
        batch drop factor  & $\uni{0}{0.5}$ \\
        hnc factor & $\loguni{-3}{-1}$ \\
        top k negative & $\floor*{\uni{1}{\mathrm{num\_classes} - 1}}$ \\
        tap factor & $\uni{0}{1}$ \\
        lambda & $\loguni{-2}{2}$ \\
        discriminator steps per generator step & $\floor*{\logunitwo{0}{3}}$ \\
        grad penalty & $\loguni{-2}{1}$ \\
        beta1 & $[0.0, 0.5]$ \\
        mlp width & $\logunitwo{6}{10}$ \\
        mlp depth & $[3, 4, 5]$ \\
        mlp dropout & $[0, 0.1, 0.5]$ \\
        \bottomrule 
    \end{tabular}
    \caption[Hyperparameters and distributions used for the mask ablation study]{Hyperparameters and distributions used in random search for the mask ablation study. $\logunix{a}{b}$ denotes a log-uniform distribution between $a$ and $b$ for base $x$, the uniform distribution is denoted as $\uni{a}{b}$, $\floor*{\cdot}$ is the floor operator, and $[\cdot]$ represents a choice.}
    \label{tab:abl-distributions-mask}
\end{table}